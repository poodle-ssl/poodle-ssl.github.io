<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>PooDLe</title>
  <link rel="icon" type="image/x-icon" href="static/images/poodle.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">PooDLe<img src="static/images/poodle.ico">: Pooled and dense self-supervised learning from naturalistic videos</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                  <span class="author-block">
                    <a href="https://alexnwang.github.io/" target="_blank">Alex N. Wang</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://2016choang.github.io/" target="_blank">Chris Hoang</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://www.cs.toronto.edu/~yuwen/" target="_blank">Yuwen Xiong</a>,</span>
                  <span class="author-block">
                    <a href="http://yann.lecun.com/" target="_blank">Yann LeCun</a>,</span>
                  <span class="author-block">
                    <a href="https://mengyeren.com/" target="_blank">Mengye Ren</a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <!-- <span class="author-block"><small><sup>1</sup>New York University, <sup>3</sup>MetaAI FAIR</small></span> -->
                      <!-- <br>Conferance name and year</span> -->
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                  </a>
                </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Self-supervised learning has driven significant progress in learning from single-subject iconic images. However, there remain unanswered questions regarding the use of minimally curated naturalistic video data, particularly concerning object size imbalance and the preservation of geometric details in learned representations. In this paper, we propose a novel approach that combines the traditional SSL objective on pooled representations with a dense spatial objective aligned by external optical flow predictions. Our findings indicate that a unified objective, extended across multiple feature scales, is essential for effectively learning about objects of varying scales present in high-resolution naturalistic videos. We validate our approach on the BDD100K driving video dataset and the WalkingTours first-person video dataset, demonstrating its ability to capture both the fine-grained understanding from a dense objective and the semantic understanding facilitated from pooled representation learning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End paper abstract -->

<!-- Problem -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title">Problem: current SSL methods rely on iconic data assumptions</h2>
          <hr style="border: 0; border-top: 1px solid lightgray;">
        </div>
      </div>
      <div class="columns is-centered image-container">
        <div class="column is-two-fifths is-centered">
          <img src="static/images/imagenet-image.jpeg" alt="Iconic image" class="image"> 
        </div>
        <div class="column is-two-fifths is-centered">
          <img src="static/images/bdd-image.png" alt="Naturalistic video" class="image"> 
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          Self-supervised learning (SSL) has been able to learn useful visual representations without manual labels.
          These methods could allow us to harness large-scale internet data such as naturalistic in-the-wild videos for training powerful vision models.
          Nevertheless, many SSL methods still revolve around the typical ImageNet dataset and may implicitly rely on its biases.
          In particular, the dataset consists of iconic images (shown above left) which contain a single central subject, and the dataset provides a balanced class distribution of subjects.
          In contrast, naturalistic videos often contain scene images (shown above right) with multiple objects, imbalanced class distributions, and varying object scales, making them potentially ill-suited for iconic-based methods.
        </div>
      </div>
      <div class="columns is-centered image-container">
        <div class="column is-two-fifths is-centered">
          <img src="static/images/bdd-semseg.png" alt="BDD semantic segmentation"> 
        </div>
        <div class="column is-two-fifths">
          <img src="static/images/class-distribution-colored.png" alt="BDD class distribution"> 
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          Iconic SSL methods typically learn representations by minimizing the distance between the pooled features of two augmented global crops of the same image.
          This approach may not be effective for multi-object scenes because the model would try to learn incorrect invariances between two augmented views which are likely to contain semantically different object instances.
          Recent works have proposed dense SSL objectives that can differentiate between different objects by learning 2D feature representations which maintain spatial information.
          However, dense SSL methods may still suffer from spatial region imbalance, where foreground objects are usually smaller than the background (shown above), resulting in poorer performance on important small object classes in downstream tasks.
          For example, in the BDD100K dataset of driving videos, traffic signs on average occupy only 0.2% of pixels per video frame and thus, may be underrepresented in dense SSL objectives yet traffic signs are crucial for self-driving applications.
          To bootstrap learning of foreground object semantics, current SSL methods for naturalistic videos still rely on globally-pooled iconic objectives or iconic datasets.
          Overall, there remains a lack of SSL methods that can leverage both dense SSL and object-centric semantic learning to effectively learn visual representations from naturalistic videos.
      </div>
  </div>
</section>
<!-- End problem -->

<!-- Method -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title">Method</h2>
          <hr style="border: 0; border-top: 1px solid lightgray;">
        </div>
      </div>
      <div class="columns is-centered image-container">
        <div class="column is-four-fifths is-centered">
          <img src="static/images/method.png" alt="Method" class="image"> 
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          To address these challenges of learning from naturalistic video, we propose PooDLe, a SSL method that combines a dense flow equivariance objective on global, paired video frames and a pooled invariance objective on local, small subcrops.
          The dense objective captures more spatial and geometric details by learning features that are equivariant to flow, or the motion between paired frames, at the global scene level.
          On the other hand, the pooled BYOL-style objective learns foreground object semantics from small subcrops, which act as <em>pseudo-iconic</em> data as the subcrop is likely to contain only a single object.
          We introduce a lightweight spatial decoder module in order to unify these two objectives within a single model architecture.
        </div>
      </div>
      <div class="columns is-centered image-container">
        <div class="column is-four-fifths is-centered">
          <img src="static/images/subcrop.png" alt="Method" class="image" style="display: block; margin-left: auto; margin-right: auto;"> 
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          We hypothesize that using small subcrops can mitigate spatial region imbalance because the percentage of subcrops that have sufficient coverage of small objects will be significantly higher than the percentage of pixels that containing those objects.
          We define <em>sufficient coverage</em> as when a subcrop contains &ge;10% of an object, which we believe is a reasonable threshold as foreground objects have higher information content compared to repetitive background textures and thus, their information is preserved in pooled representations.
          Using data generated from a toy model of subcrop coverage of foreground objects, the graph above shows that not only is the subcrop probability higher than the pixel probability, but also the relative proportion of subcrop-to-pixel probability (green labels) is much greater for smaller objects.
        </div>
      </div>
      <div class="columns is-centered image-container">
        <div class="column is-two-fifths is-centered">
          <img src="static/images/base-architecture.png" alt="Base architecture"> 
        </div>
        <div class="column is-two-fifths">
          <img src="static/images/spatial-decoder-architecture.png" alt="Spatial decoder architecture"> 
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          Consider a typical convolutional encoder network with multiple feature levels of decreasing resolution scale, such as ResNet-50.
          A naive baseline (shown above left) is to apply both objectives at the last feature level.
          However, the downsampling effect of the encoder (e.g. 32x in ResNet-50) limits the preservation of small objects and fine details for the dense objective.
          To remedy this, we introduce a spatial decoder module (shown above right) that leverages skip connections to earlier feature layers to upsample and refine the last-layer encoder features into a higher-resolution feature map to be used for the dense objective.
          We keep the pooled objective at the last encoder layer following prior SSL methods.
          Our intuition is that the last-layer encoder features serve as an information bottleneck, because they need to capture high-level object invariances to optimize the pooled objective, but also must preserve sufficient spatial information to be readily upsampled by the spatial decoder and ultimately, satisfy the dense objective.
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End method -->

<!-- Results -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title">Results</h2>
          <hr style="border: 0; border-top: 1px solid lightgray;">
        </div>
      </div>
      <div class="columns is-centered image-container">
        <div class="column is-four-fifths is-centered">
          <img src="static/images/bdd-results-table.png" alt="BDD Results" class="image"> 
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
        </div>
      </div>
      <div class="columns is-centered image-container">
        <div class="column is-four-fifths is-centered">
          <img src="static/images/groupings-results-table.png" alt="Grouping Results" class="image"> 
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-four-fifths">
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End results -->

<!-- Conclusion -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-four-fifths">
          <h2 class="title">Conclusion</h2>
          <hr style="border: 0; border-top: 1px solid lightgray;">
        </div>
      </div>
      <div class="columns is-centered">
        <div class="column is-four-fifths">

        </div>
      </div>
    </div>
  </div>
</section>
<!-- End conclusion -->


<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item">
      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{wang2024poodle,
        title={PooDLe: Pooled and dense self-supervised learning from naturalistic videos}, 
        author={Alex N. Wang and Chris Hoang and Yuwen Xiong and Yann LeCun and Mengye Ren},
        year={2024},
        eprint={TBD},
        archivePrefix={arXiv},
        primaryClass={cs.CV}
  }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
